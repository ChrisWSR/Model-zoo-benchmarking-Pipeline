# Model Zoo Benchmarking Pipeline
TODO
create a good drescription
create a better explanation
create a requirements file
Automated pipeline to download models from Hugging Face, export them to ONNX,
and benchmark **latency, throughput, and accuracy** across CPU and GPU backends.

## Quick Start

```bash
# Install
pip install -r requirements.txt

# Run the full benchmark (CPU)
python src/benchmark.py --models resnet18 resnet50 efficientnet_b0 --batch-sizes 1 4 8

# Run with GPU
python src/benchmark.py --gpu

# Generate markdown report
python src/report_generator.py --results-dir results --output RESULTS.md
```

## Project Structure

```
model-zoo-benchmark/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ benchmark.py          # Main orchestrator â€” run this
â”‚   â”œâ”€â”€ exporter.py           # PyTorch â†’ ONNX conversion + validation
â”‚   â”œâ”€â”€ model_registry.py     # All supported models live here
â”‚   â””â”€â”€ report_generator.py   # CSV â†’ Markdown tables
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_regression.py    # CI latency regression guard
â”œâ”€â”€ results/  
â”œâ”€â”€diagnos
â”‚   â””â”€â”€ check_gpu.py          # debug for gpu enablement troubles 
â”œâ”€â”€ .github/workflows/
â”‚   â””â”€â”€ benchmark.yml         # CI: runs weekly + on every PR
â””â”€â”€ requirements.txt
```

## ğŸ“Š Sample Results (batch_size=1, CPU)

> ğŸŸ¢ < 10ms &nbsp;&nbsp; ğŸŸ¡ 10â€“50ms &nbsp;&nbsp; ğŸ”´ > 50ms

| Model           | Backend    | Latency (ms)    | p95 (ms) | Throughput (s/s) | Size (MB) |
|-----------------|------------|-----------------|----------|------------------|-----------|
| efficientnet_b0 | onnx_cpu   | ğŸŸ¡ 18.3         | 20.1     | 54.6             | 20.1      |
| efficientnet_b0 | pytorch    | ğŸŸ¡ 32.7         | 35.2     | 30.6             | â€”         |
| mobilenet_v2    | onnx_cpu   | ğŸŸ¢ 9.4          | 10.2     | 106.4            | 13.6      |
| mobilenet_v2    | pytorch    | ğŸŸ¡ 18.1         | 19.8     | 55.2             | â€”         |
| resnet18        | onnx_cpu   | ğŸŸ¢ 7.2          | 8.1      | 138.9            | 44.7      |
| resnet18        | pytorch    | ğŸŸ¡ 14.8         | 16.3     | 67.6             | â€”         |
| resnet50        | onnx_cpu   | ğŸŸ¡ 22.6         | 24.9     | 44.2             | 97.8      |
| resnet50        | pytorch    | ğŸŸ¡ 43.1         | 46.2     | 23.2             | â€”         |

*ONNX Runtime delivers **1.7â€“2.4Ã— lower latency** vs PyTorch on CPU across all tested models.*

## ğŸ› ï¸ Adding a New Model

Edit `src/model_registry.py` and add an entry:

```python
MODEL_REGISTRY["your_model"] = {
    "name": "your_model",
    "hf_id": "org/model-name",   # Hugging Face model ID
    "task": "classification",
    "img_size": 224,
    "opset": 17,
}
```

Then run `python src/benchmark.py --models your_model`.

## Key Design Decisions

**Dynamic batch axes in ONNX export** â€” allows a single exported model to serve any batch size without re-export.

**Graph optimization at ORT level** â€” `ORT_ENABLE_ALL` applies constant folding, operator fusion, and layout optimization automatically.

**Warmup iterations** â€” 10 warmup passes before measurement eliminate JIT compilation and cache cold-start noise.

**p95/p99 latency** â€” mean latency alone is misleading; tail latency matters for real serving SLAs.

**CI regression guard** â€” `test_regression.py` fails the pipeline if any model exceeds the configured ms threshold, preventing silent perf regressions on new PRs.

##  Roadmap

- [ ] LLM benchmarking (TinyLlama, Phi-2) â€” token/s, TTFT, memory footprint
- [ ] INT8 quantization via ONNX Runtime quantization API
- [ ] YOLOv8 object detection task support
- [ ] GPU (CUDA) benchmarking in CI via GitHub-hosted GPU runners
- [ ] Accuracy evaluation on ImageNet-1k validation subset
- [ ] Streamlit dashboard for interactive result exploration